---
output:
  pdf_document: default
  html_document: default
  word_document: default
---
---
title: "Statistical Learning Project"
author: " Filippo Santin, Gurjeet Singh, Francesca Zen"
date: "18/5/2021"
output:
  pdf_document: default
  html_document: default
---
# 1 Introduction

In the following report, we present an analysis computed on stroke disease, and we try to explain from statistical analysis some correlation factors and statistics of the given features/predictors by designing models to predict the presence of stroke disease. In addition, we highlight possible linear and non-linear relationships among the given features (predictors) and the stroke disease variable (predicted variable).  
"Stroke" is the medical term for damage to brain tissue or the death of a portion of it, due to insufficient blood supply to an area of the brain.  According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths and our aim is to see if and how the variables we are dealing with are related, in order to predict which individual is more probable to have a stroke.  
The symptoms of stroke vary from patient to patient, depending on the severity of the condition, the affected brain area, causes, type of stroke, etc. Stroke is characterized by sudden onset and for this reason it involves the need for immediate therapeutic intervention and adapted to the needs of the patient. In this sense, looking for relation between features may help to prevent or assess it.

In order to have a guide for the interpretation of the data we underline the following information:

* The normal values of glucose level are between 60 and 110 mg/dl and with a value greater than 126 mg/dl a person is considered diabetic;
* a body mass index (BMI) between 18.5-24.9 indicates a normal/healthy weight, below 18.5 indicates underweight, 25.0-29.9 indicates overweight and above 30.0 indicates obese person.

# 2 Exploring the Dataset

The dataset we used is provided by Kaggle ^[https://www.kaggle.com/fedesoriano/stroke-prediction-dataset] and it is composed of 5,110 entries with a total of 12 columns: `id`, `gender`, `age`, `hypertension`, `heart_disease`, `ever_married`, `work_type`, `Residence_type`, `avg_glucose_level`, `bmi`, `smoking_status`, `stroke`.
```{r}
library(knitr)
stroke_data <- read.csv('healthcare-dataset-stroke-data.csv')
kable(stroke_data[1:5,], format = 'simple', align='ccccccccc', 
      col.names = c('id','gender','age', 'hypert.', 'hd' ,'ev_marr',
                    'work_type','res_type','glucose', 'bmi','smoking','stroke'))
```

## 2.1 Preprocessing

The preliminary part of the analysis focuses on the study of the dataset and its pre-processing: we looked at the `id` column and verified that all the data collected was referring to different people, thus no recidivist status were involved.  
After this check we removed the column from the dataset since it did not hold useful information for our study.
```{r}
stroke_data <- stroke_data[,-1]
```

In order to use the variables through the analysis we transformed the categorical variables into factors:

```{r}
stroke_data$gender <- as.factor(stroke_data$gender)
stroke_data$ever_married <- as.factor(stroke_data$ever_married)
stroke_data$work_type <- as.factor(stroke_data$work_type)
stroke_data$Residence_type <- as.factor(stroke_data$Residence_type)
stroke_data$smoking_status <- as.factor(stroke_data$smoking_status)
```

The variable `bmi` was not numeric because of the presence of  "N/A" string values which identify missing information, hence we transformed its elements into numerical values and then removed the NA values generated.

```{r, results='hide', message=FALSE}
stroke_data$bmi <- as.numeric(stroke_data$bmi)
stroke_data <- na.omit(stroke_data)
```

We ended up having  4,909 entries and 11 total columns.  
Here we give a quick overview of the main information about the dataset:
```{r}
summary(stroke_data)
```

## 2.2 Descriptive Statistic

```{r, results='hide', message=FALSE}
attach(stroke_data)
```

In order to highlight and study better the data, we used some plots to study their statistics and distribution.  
A relevant and important information is provided by the following barplot, in which we see an unbalance dataset issue: $209$ people on a total of $4909$ get a stroke, i.e. the $4.25 \%$ of the people.
```{r, fig.height=3, fig.width=3, fig.align='center'}
table(stroke)
table(stroke)/dim(stroke_data)[1]
barplot(table(stroke)/dim(stroke_data)[1],
        xlab='stroke',col = c('#F8766D','#00BFC4'))
```
These values are representative of the real situation in which there are not many stroke cases compared with the whole population, the people who have had a stroke are much less than the ones who did not have it. The incidence of stroke in Europe at the beginning of the 21st century varies from $95$ to $290$ cases/$100,000$^[QUADERNI dell'Italian Journal of Medicine, A Journal of Hospital and Internal Medicine, Michele Meschi,volume 8, issue 2, March-April 2020]. Furthermore in many clinical disease analyses this issue is commonly present.

A visual transformation of the values seen in the `summary` function is provided in the following boxplots:
```{r bp, fig.height=3.5, fig.width=6.5, fig.cap='\\label{bp}Visual description of some features of the dataset'}
par(mfrow=c(1,3))
boxplot(avg_glucose_level, xlab= 'average glucose level' , col='#00BA38')
boxplot(bmi, xlab = 'body mass index', col='#00BA38')
boxplot(age, xlab = 'age', pch=20, col='#00BA38')
par(mfrow=c(1,1))
```

From Figure \ref{bp} we can see that in the first two boxplots (starting from the left) there are lots of outliers, that can also be seen from the summary by looking at the difference between the third quantile and the maximum value in the `avg_glucose_level` and `bmi` variables.  
Actually, they represent real-case scenarios (people affected by high glucose levels or with a bmi out of range are only a few) and possible interesting cases of pathologies bounded with diabetes. Hence these data points have to be considered during modeling, they could be helpful to predict stroke cases because it could be due to complications of diabetes, as mentioned in the medical literature.

In order to compare features in pairs and judge which of each one is preferred, or has a greater amount of some quantitative property we provide a pair-wise plot. In addition, to involve also the categorical variables we wrote some useful functions:
```{r}
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...){
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- abs(cor(x, y))
  txt <- format(c(r, 0.123456789), digits = digits)[1]
  txt <- paste0(prefix, txt)
  if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
  text(0.5, 0.5, txt, cex = cex.cor * r)
}

panel.hist <- function(x, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col = "green", ...)
}

box_plot_categories <- function(data, y){
  n_features = length(data)
  grid = round(sqrt(n_features))
  print(grid)
  par(mfrow=c(grid, grid))
  names = colnames(data)
  for (idx in c(1:n_features)) {
    plot(y ~ data[, idx], xlab=names[idx], main=c('Boxplot y ~ ', names[idx]))
  }
  par(mfrow=c(1, 1))
}
```

And here we show the results from the pairs plot:
```{r, fig.dim=c(6,6)}
pairs(stroke_data, diag.panel=panel.hist, upper.panel=panel.cor)
```

The pairs-plot shows that the stronger relationships involve quite often the variable `age`. There are also other relevant relations, for example between `work_type` and `ever_married` or `bmi` with `working_status`.
In addition, we can see strong collinearity among the dummy variables `age`, `work_type`, and `ever_married`, this mean that they are closely related to one another. For this reason we get uncertainty in the coefficient estimates and so we do not consider them in the fitting of the models.  
We go on looking at some intuitive relationship of `stroke` with `age`,`bmi` and `avg_glucose_level`:
```{r bp_stroke, fig.height=3.5, fig.width=6.5, fig.cap='\\label{bp_stroke}Visual rappresentation of the comparison between `stroke`(1) and not-`stroke`(0) of the features: `avg_glucose_level`, `bmi` and  `age`'}
par(mfrow=c(1,3))
boxplot(avg_glucose_level~stroke, xlab= 'stroke', 
        ylab = 'average glucose level', col = c('#F8766D','#00BFC4'))
boxplot(bmi~stroke, xlab = 'stroke', ylab = 'bmi', col = c('#F8766D','#00BFC4'))
boxplot(age~stroke, xlab='stroke' ,ylab = 'age', col = c('#F8766D','#00BFC4'))
par(mfrow=c(1,1))
```

Looking at Figure \ref{bp_stroke} we can see that the incidence of the disease increases progressively with age, in particular the summary of `age` shows that it has been computed a sampling between a baby of 8 days and a senior of 82 years old and this can also be seen in the boxplot. We get also the information that the youngest person affected of stroke disease is 14 years old (an outlier), while the oldest one is 82 years old.  
We make a more detailed analysis looking at the following tables:
```{r}
table(stroke.less.35 <- stroke_data[stroke_data$age<35, 'stroke'])
table(stroke.35.50 <- stroke_data[stroke_data$age>=35 & stroke_data$age<50 , 'stroke'])
table(stroke.major.50 <- stroke_data[stroke_data$age>=50 , 'stroke'])
```
In `stroke.less.35` we can notice that there are only two person under the age of 35 which is suffered a stroke illness, while in `stroke.35.50` we highlight the number of people between the age of 35 and 50 that are ill, i.e. 16. The major difference between the numbers can be seen in `stroke.major.50` where stroke patients older than 50 are much more than the one of the previous cases, so we can conclude that an increment on the age strongly contribute on the probability to get a stroke.  
If we sum to `age` the information about `avg_glucose_level` we may wonder if diabetic people are more probable to get a stroke or not. On the other hand, there is no apparent relation of `stroke` with `bmi`.

We now highlight other visual relationship between the variables used before:
```{r,fig.height=3, fig.width=4.3, fig.align='center'}
library(ggplot2)
ggplot(stroke_data, aes(x = avg_glucose_level, y = bmi,col = as.factor(stroke))) +
  labs(x = "average glucose level", y = "bmi", color = "Stroke") + geom_point()
ggplot(stroke_data, aes(x = avg_glucose_level, y = age,col = as.factor(stroke))) + 
  labs(x = "average glucose level",y = "age", color = "Stroke") + geom_point()
ggplot(stroke_data, aes(x = bmi, y = age, col = as.factor(stroke))) +
  labs(x = "bmi", y = "age", color = "Stroke") +geom_point()
```

What we can see from these scatter plots is that if we have low `avg_glucose_level` values in relation with `age` or `bmi`, the stroke disease is not present. On the other hand, if we look at the `bmi` predictor we can notice that there are samples of stroke cases spread all over in all of its range of values, so it highlights no clear correlation of `bmi` with stroke, even when we put this features in relation with the others.  
`avg_glucose_level` and `bmi` could not be so strictly related to disease but maybe correlated to other illnesses linked (or not) to it.  
In the end we can see that it is not easy to identify the direct relationship with the stroke while dealing with the features that we have, and it is difficult a further interpretation of the data.

## 2.3 Data Science Questions

At this point we can ask some questions:

* Which factors are the most related to the stroke disease?
* How strong are the relations between the features?
* Are the given variables enough to predict a good accuracy of some possible person affected by stroke?
* Is it possible to prevent it?

We will explore the data trying to answer these questions.

# 3 Modeling

In order to explore different approaches for predicting the qualitative response `stroke`, in the following paragraph we will cover three of the most widely-used classifiers: *logistic regression*, *linear discriminant analysis (LDA)* and *quadratic discriminant analysis (QDA)*. The prediction process is known as *classification*.  

## 3.1 Logistic Regression

Logistic regression can be defined as a type of generalized linear model (GLM). In particular, it is a statistical model used to examine the association of categorical or continuous independent variables with one dichotomous dependent variable.  
Rather than computing directly the response $Y$, logistic regression computes the probability that $Y$ belongs to a particular category, in our case $1$ if the person has a stroke disease, $0$ otherwise.  
In this part of the predictive analysis we will present three different type of models, compared to discover the best one that can better interpret the data. We will apply the Akaike Information Criterion (AIC) as selection of the candidate models (given directly from the `summary` function).  
We decide to exclude the Mallow's Cp and the R^2 adjusted techniques because they work efficiently in the case of linear regression.



### 3.1.1 Full and Reduced Models

We start with the full model to see if all the features of the dataset contribute on the prediction of a stroke.  
Up to now we have already mentioned if some predictors seem to be more or less correlated to the response variable, and a way for analyze such relation is by introducing the null hypothesis. In the case of the full model, we are dealing with $10$ explanatory variables and so we have 
$$H_0 : \beta_1 = \dots = \beta_{10} = 0$$
and this means that we are making the assumption that no predictors is somehow related with `stroke`. If we reject the null hypothesis we are assuming that there is in fact a correlation and we have to find which feature is involved.  
To decide whether or not reject $H_0$ we look at the p-values: in our case we decide to put $\alpha = 0.1$, for p-values smaller than $\alpha$ we keep the feature, otherwise we remove it from the model. \textcolor{red}{This means that we will end up with a confidence interval of $90\%$ in the classification.}  
Let's have a look now at the characteristics of the full model:
```{r, fig.dim=c(3.5,3.5)}
mod.full <- glm(stroke~., data = stroke_data, family = binomial)
summary(mod.full)
```
Here we see that `age`, `avg_glucose_level` and `hypertension` are the variables most related to `stroke`, with a p-value smaller than $0.005$, also `heart_disease` contributes a bit to the model with a p-value of $0.092381$.  
Let's use the residual plots to get more information about this model (we use `type="deviance"` because we have a binary response).
```{r}
par(mfrow=c(2,2))
plot(mod.full)
par(mfrow=c(1,1))
```

The plots are not satisfactory because they are not easy to interpret. It is simply evident that data do not follow a linear trend (we will discuss more in detail in the next paragraphs).

We now go on with some tests, we take the full model and we remove all the predictors that show up collinearity between each other, i.e. `work_type`, `Residence_type` and `ever_married`, following a sort of greedy backward selection:
```{r}
mod.red1 <- glm(stroke ~ age + bmi + avg_glucose_level + hypertension + 
                  smoking_status + gender + heart_disease, family=binomial)
summary(mod.red1)
```

Also in this case the significant covariates are the same of the ones found in the full model.  
In this step we remove the `gender` variable:
```{r}
mod.red2 <- glm(stroke ~ age + bmi + avg_glucose_level + hypertension + 
                  smoking_status  + heart_disease, family=binomial) 
summary(mod.red2)
```

The parameters left to eliminate are `bmi` and `smoking_status`.  
We are now able to define the final reduced model:
$$\verb|stroke| = \beta_0 + \beta_1\times \verb|age| + \beta_2\times \verb|heart_disease| + \beta_3 \times  \verb|avg_glucose_level| + \beta_4 \times\verb|hypertension|$$
with four explanatory variables.
```{r}
mod.red <- glm(stroke~age + avg_glucose_level + heart_disease + hypertension, 
               data=stroke_data, family = binomial)
summary(mod.red)
```
An important thing to notice is that there has been a decreasing on the p-value of the explanatory variable `heart_disease`, that now have a p-value of $0.046895$.  

This means that our probability to get a stroke becomes:
$$p(X)= \frac{e^{\beta_0+\beta_1X_1 + \beta_2X_2+\beta_3 X_3+\beta_4 X_4}}{1+e^{\beta_0+\beta_1X_1 + \beta_2X_2+\beta_3 X_3+\beta_4 X_4}}$$
where $\beta_0, \dots, \beta_4$ can be read in the `summary` of `mod1` with the respectively variables $X_1, \dots, X_4$ which represent `age`, `avg_glucose_level`, `heart_disease` and `hypertension`.  
From $p(X)$ we can get the quantity $p(X)/(1-p(X))$ which is called *odds*, and it can take any value between $0$ and $\infty$. Values of odds close to $0$ and $\infty$ indicates very low and very high probabilities to get a stroke, respectively.  
In our logistic regression model, increasing $X_i$ by one unit multiplies the odds by $e^{\beta_i}$, for $i=1, \dots, 4$. The amount of change in $p(X)$ due to a one-unit change in $X_i$ depends on the current value of $X_i$. But regardless of the value of $X_i$, if $\beta_i$ is positive than increasing $X_i$ will be associated with increasing $p(X)$, and if $\beta_i$ is negative than increasing $X_i$ will be associated with decreasing $p(X)$, for $i=1, \dots, 4$.  
The fact that there is not a straight-line relationship between $p(X)$ and $X_i$, and the fact that the rate of change in $p(X)$ per unit change in $X_i$ depends on the current value of $X_i$

For this model we also give a descriptive statistic through the residual plots:
```{r}
par(mfrow=c(2,2))
plot(mod.red)
par(mfrow=c(1,1))
```

Reading and interpreting these graphs, we can state some important outcomes:

**Residuals vs Fitted**  
We can observe the presence of high non-linearity in the dataset.
Indeed we can find not spread equal residuals around a horizontal line without distinct patterns, attesting to the fact that there is no linear relationship. 

**Normal Q-Q**  
We discover that residuals do no follow a normal distribution.  
We know that obtaining positive coefficient estimates, we have a positive association. So, the larger the value the higher is the estimated probability of stroke. 

**Scale-Location**  
We can see that homoscedasticity does not hold since the line of the standard residual is not flat, hence even by standardizing the residual we end up having high variance among residuals. But in our case, as you can notice from this plot, the red line is slightly curved and the residuals increase as the fitted Y values increase. So, the inference here is, heteroscedasticity exists.

**Residual vs Leverage**  
Looking at the leverage plot, we see the presence of some samples with high leverage value (bottom right), which could influence the prediction of the model (even if the R-plot does not show us its index).   
Moreover, there are outliers (they have no connection with our regression) with high variance and they represent some rare cases of disease development; as we saw previously there was a case of a 14 years old girl that had stroke.
```{r, echo=FALSE}
kable(stroke_data[c("119","183","246"),], format = 'simple', align='cccccccc', 
      col.names = c('gender','age', 'hypert.', 'hd' ,'ev_marr',
                    'work_type','res_type','glucose', 'bmi','smoking','stroke'))
```

If we compare the results, we can affirm that the reduced model seems to make a more accurate prediction and fits better data than the full one. Indeed, its AIC is the lower of the two.  
To have a confirmation of this concept, we use 'anova', a function which compares them and returns the best:
```{r}
anova(mod.full, mod.red, test="Chisq")
```
As expected, the anova test rejects that the full model is more pertinent than the reduced one, since the p-value of its features are not less than $5\%$ or even $10\%$. Therefore the full model does not enhance the prediction.

### 3.1.2 Interaction Models

Let's try to check other convenient models using a mixed approach. We use the reduced model `mod.red` as landmark and we proceed with interactions between the explanatory variables with the aim to understand which variables are relevant on our research and to improve the performance of the model. We remember that we move on with our selection choosing significance levels equal to $0.1$ and searching for an AIC value less and less.  
The model `mod.red` has `stroke` as response and `age`, `avg_glucose_level`, `hypertension` and `heart_disease` as predictors.
We recall that its AIC is $1384.6$.

Initially we consider the interaction of `age` with the other numerical features, i.e. `avg_glucose_level`, `hypertension` and `heart_disease` and we verify that only `age*heart_disease` gives a relevant contribution. We get AIC = 1384. 
```{r}
mod1 <- glm(stroke~age + avg_glucose_level + heart_disease+ hypertension +
               age*heart_disease, family=binomial)
summary(mod1)
```
We considered all the interaction of `avg_glucose_level` with the remaining predictors and found out that `avg_glucose_level*hypertension` was the best of the possible relation even if it can not improve the previous model.  
It has an AIC of 1385.9.
```{r}
mod2 <- glm(stroke~age + avg_glucose_level + heart_disease+hypertension +
              avg_glucose_level*hypertension, family=binomial)
summary(mod2)
```
Ultimately, we have the interaction `heart_disease*hypertension` which shows a good version and returns an AIC 1384.5 for the model.
```{r}
mod3 <- glm(stroke~age + avg_glucose_level + heart_disease+hypertension + 
            heart_disease*hypertension, family=binomial)
summary(mod3)
```

After that, we picked up the reduced model without `heart_disease` which represents the explanatory feature with higher p-value and we continue to work in the same way we have just seen. We repeated the scheme with `age`, without earning benefits.  
The AIC has a value of 1386.2.
```{r, results='hide'}
mod4 <- glm(stroke ~ age + avg_glucose_level + hypertension + 
              age*hypertension, family=binomial)
summary(mod4)
```
We go further analyzing also `avg_glucose_level*hypertension`.
```{r, results='hide'}
mod5 <- glm(stroke ~ age + avg_glucose_level + hypertension + 
              avg_glucose_level*hypertension, family=binomial)
summary(mod5)
```
In the end, we included the possible best interactions into the reduced model.
It could be a good way, in fact it does not represent our best solution due to a high p-value.  
The corresponding AIC is 1384.2.
```{r, results='hide'}
mod6 <- glm(stroke ~ age + avg_glucose_level + heart_disease+ hypertension +
              age*heart_disease + heart_disease*hypertension, family=binomial)
summary(mod6)
```
As conclusion, we can promote `mod1` as the model which better fits our data:
$$\verb|stroke| = \beta_0 + \beta_1\times \verb|age| + \beta_2\times \verb|avg_glucose_level| + \beta_3 \times \verb|heart_disease| + \beta_4 \times\verb|hypertension| + \beta_5 \times \verb |age*heart_disease|$$
Let's now see some relevant information, such as outliers on the `mod1`:
```{r, echo=FALSE}
kable(stroke_data[c("207","150","100"), ], format = 'simple', align='cccccccc', 
      col.names = c('gender','age', 'hypert.', 'hd' ,'ev_marr',
                    'work_type','res_type','glucose', 'bmi','smoking','stroke'))
```

and other properties and characteristics:
```{r}
par(mfrow=c(2,2))
plot(mod.full)
par(mfrow=c(1,1))
```

### 3.1.3 Polynomial models

Let's try to do one more test using the polynomial model starting from the reduced model `mod.red`, adding also `bmi` as predictor. We provide the square of `bmi`, `avg_glucose_level` and then both of them, obtaining `mod.poly1`, `mod.poly2` and `mod.poly3` respectively.

```{r, results='hide'}
mod.poly1 <- glm(stroke ~ age + heart_disease + avg_glucose_level+ hypertension+bmi+
                       I(bmi^2), family = binomial)
mod.poly2 <- glm(stroke ~ age + heart_disease + avg_glucose_level+ hypertension+bmi+
                     + I(avg_glucose_level^2), family = binomial)
mod.poly3 <- glm(stroke ~ age + heart_disease + avg_glucose_level+ hypertension+bmi+
                      I(bmi^2) + I(avg_glucose_level^2), family = binomial)
```
However, we do not achieve anything interesting with polynomial model. There are no improvements in the results, seeing the value of the AIC which is extremely high.

## 3.2 Bayesian modelling 
In this section we now study other types of predictive models, which are based on Bayesian concepts. These methods exploit conditional probability and Bayes theorems to make predictions, they are also widely used in classification problems because of their nice structure and properties. Furthermore, we would like to compare their results with the logistic model in order to assess their performances. Recall that we have to consider the LDA and QDA models which approximate the Bayesian classifier (the ideal one) as from a computation point of view it is very expensive and requires that the likelihood and the prior distribution to be conjugate (i.e their distributions are from the same family of distribution).

### 3.2.1 LDA
We first consider the Linear Discriminant Analysis (LDA) by taking into account the Multivariate case. When we apply LDA model to approximate the Bayesian classifier we have assumed that the likelihood follows a Normal distribution $X|G_j \sim \mathcal{N}(\mu_j,\,\Sigma) \quad j=0, 1$, and covariates with same $\Sigma$ for both classes. Hence we have:
$$P(G_j | X) \propto \pi_j \; \frac{1}{2\pi |\Sigma|^{1/2}} \exp \left(-0.5 (X - \mu)^\top \Sigma^{-1} (X-\mu) \right) \quad j=0,1 $$
Where the parameters $\pi, \mu, \Sigma$ are plugged in with their associated estimator: $$\hat\pi = \frac{n_j}{n}\,, \qquad \quad\hat\mu = \overline x_j\,, \qquad \quad\hat\Sigma = \sum_{j=1}^{2} (X_i-\mu_j)^\top (X_i-\mu_j) / (n-2).$$
The LDA furthermore will assign for each sample the class with the largest posterior probability using the discrimination score.
In the following code we fit the data by considering all the predictors independent to each other, in order to not have collinearity issue.
```{r}
library(MASS)
lda.fit <- lda(stroke ~ age + bmi + avg_glucose_level + hypertension +  gender
               + smoking_status +  Residence_type + heart_disease)
lda.pred <- predict(lda.fit)
lda.pred.stroke <- lda.pred$posterior[, 2]
table(lda.pred$class, stroke)
```
Since we are dealing with an unbalanced dataset we are not satisfied with this criteria of assigning the class, thus in chapter 4 we will cover a better technique to define the best posterior threshold.

### 3.2.2 QDA
Assuming homoscedasticity among the classes can be very restrictive in our case since the dataset is very unbalanced and skewed. Thus we considered also the Quadratic Discriminant Analysis (QDA) which estimates $\mu_j$ and $\Sigma_j$ for each class separately. This choice will turn out to have a more flexible and reliable model than the previous one
```{r}
qda.fit <- qda(stroke ~ age + bmi + avg_glucose_level + hypertension + heart_disease 
               + smoking_status, data = stroke_data)
qda.pred <- predict(qda.fit, stroke_data)
qda.pred.stroke <- qda.pred$posterior[, 2]
table(qda.pred$class, stroke)
```
As previously we will need to cover better the threshold of the QDA model due to the unbalanced data.

# 4 Best Model Selection & Validation Test
In this chapter of the report we are now going to evaluate our elective models that we have seen so far and pick the best among them. Furthermore we will also discuss the metrics used to define the best classification threshold of the predicted variables.
The models that we elected for this analysis, which have shown to represent better our problem are: the two logistic regressions (reduced and interaction based), and the models built with the Bayesian modelling approach (LDA and QDA).  
In order to assess them we used the validation testing method, hence we split the dataset into validation and training sets. But since our dataset is highly unbalanced we need to define the appropriate split in order to have reasonable amount of stroke cases in both sets.  
We also recall that the data is made of 4909 instances and just the 4.25% of the persons are affected by the stroke. We decided then to keep the 75% of the data for the training data: 3682 samples of which 3562 are the ones that have not incurred to a stroke, while 120 of them had it. The remaining data are part of the validation set.

Cross-validation tests have not been developed since constructing disjoint splits and random sampling from the data is not feasible with just few cases of stroke from the whole dataset.

## 4.1 Data split
In the following R code we developed the above mentioned splitting and sampling methodology.

First we retrieved the shuffled indices of the person which are affected and not by the stroke in different variables.
```{r}
no.strokes.data <- stroke_data[stroke == 0, ]
rnd.idx.no.strokes <- sample(c(1:dim(no.strokes.data)[1]))

yes.strokes.data <- stroke_data[stroke == 1, ]
rnd.idx.yes.strokes <- sample(c(1:dim(yes.strokes.data)[1]))
```

We construct the training set made of 75% instances of persons that are not affected by the stroke, and the 57% of the stroke cases.
```{r}
training.set <- no.strokes.data[rnd.idx.no.strokes[1:3562], ]
training.set <- rbind(training.set, yes.strokes.data[rnd.idx.yes.strokes[1:120], ])
shuffle <- sample(nrow(training.set))
training.set <- training.set[shuffle, ]
```
The remaining data is then used for the validation set and it is made of 25% of person with no stroke and 42% with stroke.
```{r}
val.set <- no.strokes.data[rnd.idx.no.strokes[3563:4700], ]
val.set <- rbind(val.set, yes.strokes.data[rnd.idx.yes.strokes[121:209], ])
shuffle <- sample(nrow(val.set)) 
val.set <- val.set[shuffle, ]
```

## 4.2 ROC and PRECISION-RECALL Curves
Due to the high unbalance stroke rate defining the appropriate threshold on the predicted variable helps to overcome this issue and improves the results of the model. Note that all the models used until now give a probability output for each class, hence we will analyze the thresholds based on those output.  
Since we are dealing with a clinical problem the presence of false negative (FN) cases is more critical than having false positive (FP) ones, for this reason we considered not just the ROC curve but also the Recall-Precision curve for each model.

In order to make all these analyses we built a function that takes a list of model predictions, for each of them we evaluate the ROC and the Precision-Recall curves and we retrieve the best threshold of both curves in a data frame table, furthermore plot of the curves are also shown.  
The best threshold for ROC curves corresponds to the value that maximize more the true positive rate and minimize the false positive rate. Instead for the Precision-Recall curve case we look for the value that minimize more the false negative rate (high recall value) and maximize also the precision, therefore we recover the best value by calculating the minimum value obtained by summing the recall and precision pairs.
```{r}
library(pROC)
library(ROCR)
get.roc.recall.values <- function(pred_models, true_value, yes_plot=TRUE) {
  result <- data.frame(Thr.ROC=double(), Specificity=double(), Sensitivity=double(),
                       Thr.Prec.Rec=double(), Recall=double(), Precision=double())
  n_models = length(pred_models)
  par(mfrow=c(n_models, 2))
  for (pred in pred_models) {
    ### ROC
    roc.res <- roc(true_value, pred, levels=c("0", "1"))
    if (yes_plot==TRUE){
      plot(roc.res, print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", 
           ylab="True positive rate")
    }
    best.roc  <- coords(roc.res, "best")
    ###
    
    ### PREC-REC
    pred.rec = prediction(pred, true_value)
    perf = performance(pred.rec, "prec", "rec")
    if (yes_plot==TRUE){
      plot(perf)
    }
    pr_cutoffs <- data.frame(cutrecall=perf@alpha.values[[1]], recall=perf@x.values[[1]], 
                             precision=perf@y.values[[1]])
    pr_cutoffs <- pr_cutoffs[pr_cutoffs$recall>0 &  pr_cutoffs$precision >0, ]
    best_recall <- pr_cutoffs[which.min(pr_cutoffs$recall + pr_cutoffs$precision), ]
    
    result[nrow(result) + 1,] = c(best.roc[1, 1], best.roc[1, 2], best.roc[1, 3], 
                                  best_recall[1, 1], best_recall[1, 2], best_recall[1, 3])
  }
  par(mfrow=c(1, 1))
  return(result)
}
```


## 4.3 Model evaluation

We first fit all the elective models on our training set.
```{r}
attach(training.set)
# Logistic reduced model
mod.red.train <- glm(stroke ~ age + heart_disease + avg_glucose_level +                            hypertension, data=training.set, family = binomial)
mod.red.train.pred <- predict(mod.red.train, data=training.set, type="response")

# Logistic interaction model
mod1.train <- glm(stroke ~ age + avg_glucose_level+ heart_disease + hypertension               + age*heart_disease, data=training.set, family=binomial)
mod1.train.pred <- predict(mod1.train, data=training.set, type="response")

# LDA Model
lda.fit.train <- lda(stroke ~ age + bmi + avg_glucose_level + hypertension + 
                       smoking_status + Residence_type + heart_disease, 
                       data=training.set)
lda.fit.train.pred <- predict(lda.fit.train, data=training.set)
lda.fit.train.pred <- lda.fit.train.pred$posterior[, 2]

# QDA model
qda.fit.train <- qda(stroke ~ age + bmi + avg_glucose_level + hypertension +
                       heart_disease + smoking_status, data = training.set)
qda.fit.train.pred <- predict(qda.fit.train, data=training.set)
qda.fit.train.pred <- qda.fit.train.pred$posterior[, 2]

# QDA interaction model.
qda.inter.fit.train <- qda(stroke ~ age + avg_glucose_level+ heart_disease + 
                             hypertension + age*heart_disease, data = training.set)
qda.inter.fit.train.pred <- predict(qda.inter.fit.train, data=training.set)
qda.inter.fit.train.pred <- qda.inter.fit.train.pred$posterior[, 2]
```

The next step instead involves the calculation of the best threshold by using two mentioned metrics.  
Thus we use our built-in function by passing the elective models, we then print the summary of the results and extract the thresholds of the two metrics in different variables, which will be helpful when we will need to apply them.
```{r, message=FALSE}
res = get.roc.recall.values(list(mod.red.train.pred, mod1.train.pred, 
                                 lda.fit.train.pred, qda.fit.train.pred,
                                 qda.inter.fit.train.pred),training.set$stroke,
                                 yes_plot=FALSE)
print(res)
recall_thresholds = res$Thr.Prec.Rec 
roc_thresholds = res$Thr.ROC
```

Once fitting the models, we would like to see their performance in the training set in order to have a first evaluation, hence for each prediction we apply the associated threshold returned by the built-in function.
```{r}
mod.red.train.pred.class <- as.numeric(mod.red.train.pred >= roc_thresholds[1])
table(mod.red.train.pred.class,training.set$stroke)
mod.red.train.pred.class <- as.numeric(mod.red.train.pred >= recall_thresholds[1])
table(mod.red.train.pred.class, training.set$stroke, dnn=c('ground thruth','pred'))

mod1.train.pred.class <- as.numeric(mod1.train.pred >= recall_thresholds[2])
table(mod1.train.pred.class, training.set$stroke)

lda.fit.train.pred.class <- as.numeric(lda.fit.train.pred>=recall_thresholds[3])
table(lda.fit.train.pred.class, training.set$stroke)

qda.fit.train.pred.class <- as.numeric(qda.fit.train.pred>= recall_thresholds[4])
table(qda.fit.train.pred.class, training.set$stroke)

qda.inter.fit.train.pred.class  <- as.numeric(qda.inter.fit.train.pred >= recall_thresholds[5])
table(qda.inter.fit.train.pred.class, training.set$stroke)
```
From the above result we can see a clear example of impact of the designed thresholds in the results of the models. We can see that from the first two confusion matrices the difference between applying the ROC curve and the Precision-Recall threshold.
The former tries to minimize the false positive errors by considering just the positive results, instead the latter one, as wee see from the output matrix, takes in consideration also the false negative cases. Therefore it results to minimize more the false negative rate, by allowing more errors for false positive values.  
As mentioned before in clinical cases an error committed to diagnose a disease can be very dangerous, especially for prediction problem, when instead of predicting a stroke we classify them as harmless case.  
For this reason, from now onwards, we will consider only the Recall-Precision values since we want to minimize those cases (FN) and overcome the unbalance issue.

From the above confusion matrix we can already see which model is giving the best result by looking at the FP and FN values for each scenario, in particular we define the optimum choice which minimize more both values but by giving more weight to false negative. 


## 4.3 Validation Set

Models on validation
```{r}
mod.red.val <- predict(mod.red.train, val.set, type="response")
mod.red.val.class <- as.numeric(mod.red.val >= recall_thresholds[1])
table(mod.red.val.class, val.set$stroke)

mod1.val <- predict(mod1.train, val.set, type="response")
mod1.val.class <- as.numeric(mod1.val >= recall_thresholds[2])
table(mod1.val.class, val.set$stroke)

lda.val <- predict(lda.fit.train, val.set)
lda.val <- lda.val$posterior[, 2]
lda.val.class = as.numeric(lda.val >= recall_thresholds[3])
table(lda.val.class, val.set$stroke)

qda.val <- predict(qda.fit.train, val.set)
qda.val <- qda.val$posterior[, 2]
qda.val.class <- as.numeric(qda.val >= recall_thresholds[4])
table(qda.val.class, val.set$stroke)

qda.inter.val <- predict(qda.inter.fit.train, val.set)
qda.inter.val <- qda.inter.val$posterior[, 2]
qda.inter.val.class <- as.numeric(qda.inter.val >= recall_thresholds[5])
table(qda.inter.val.class, val.set$stroke)

```

The interaction qda, which has the same variable of the logistic interaction model,
improved a lot the result in the training set, but in the validation test 
it does perform the same as the previous qda

# 5 Conclusions and Further Analysis

Si stima che la percentuale di persone che possono avere un ictus andrà via via 
crescendo dal momento che l'età media della popolazione è in costante crescita.

Risposta alle domande ed eventualmente riferimenti ai plot.

Consigli di aggiungere features
