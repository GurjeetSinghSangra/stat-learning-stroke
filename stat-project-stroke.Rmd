---
output:
  pdf_document: default
  html_document: default
---
---
title: "Statistical Learning Project"
author: " Filippo Santin, Gurjeet Singh, Francesca Zen"
date: "18/5/2021"
output:
  pdf_document: default
  html_document: default
---
# 1 Introduction

In the following report, we present an analysis computed on stroke disease, and we try to explain from statistical analysis some correlation factors and statistics of the given features/predictors by designing predictive models to predict the presence of stroke disease. In addition, we highlight possible linear and non-linear relationships among the given features (predictors) and the stroke disease variable (predicted variable).

"Stroke" is the medical term for damage to brain tissue or the death of a portion of it, due to insufficient blood supply to an area of the brain.

Our aim is to see if and how the variables we are dealing with are related, in order to predict which individual is more probable to have a stroke.

The symptoms of stroke vary from patient to patient, depending on the severity of the condition, the affected brain area, causes, type of stroke, etc.

Stroke is characterized by sudden onset and for this reason it involves the need for immediate therapeutic intervention and adapted to the needs of the patient.
In this sense, looking for relation between features may help to prevent or assess it.

In order to have a guide for the interpretation of the data we underline the following information:

* The normal values of glucose level are between 60 and 110 mg/dl and with a value greater than 126 mg/dl a person is considered diabetic;
* a body mass index (BMI) between 18.5-24.9 indicates a normal/healthy weight, below 18.5 indicates underweight, 25.0-29.9 indicates overweight and above 30.0 indicates obese person.

# 2 Exploring the Dataset

The dataset we used is provided by Kaggle ^[https://www.kaggle.com/fedesoriano/stroke-prediction-dataset] and it is composed of 5,110 entries with a total of 12 columns: `id`, `gender`, `age`, `hypertension`, `heart_disease`, `ever_married`, `work_type`, `Residence_type`, `avg_glucose_level`, `bmi`, `smoking_status`, `stroke`.
```{r}
library(knitr)
stroke_data <- read.csv('healthcare-dataset-stroke-data.csv')
kable(stroke_data[1:5,], format = 'simple', align='ccccccccc', 
      col.names = c('id','gender','age', 'hypert.', 'hd' ,'ev_marr',
                    'work_type','res_type','glucose', 'bmi','smoking','stroke'))
```

## 2.1 Preprocessing

The preliminary part of the analysis focuses on the study of the dataset and its pre-processing: we looked at the `id` column and verified that all the data collected was referring to different people, thus no recidivist status were involved.
After this check we removed the column from the dataset as it does not hold useful information for our study.
```{r}
stroke_data<-stroke_data[,-1]
```

In order to use the variables through the analysis we transformed the categorical variables into factors:

```{r}
stroke_data$gender<- as.factor(stroke_data$gender)
stroke_data$ever_married<-as.factor(stroke_data$ever_married)
stroke_data$work_type<-as.factor(stroke_data$work_type)
stroke_data$Residence_type<-as.factor(stroke_data$Residence_type)
stroke_data$smoking_status<-as.factor(stroke_data$smoking_status)
```

In addition, the variable `bmi` was not numeric because of the presence of  "N/A" string values which identify missing information, hence we transformed it into numeric values and then removed the NA values generated.

```{r, results='hide', message=FALSE}
stroke_data$bmi <- as.numeric(stroke_data$bmi)
stroke_data<- na.omit(stroke_data)
```

We ended up having  4,909 entries and 11 total columns.
Here we give a quick overview of the main information about the dataset:
```{r}
summary(stroke_data)
```

## 2.2 Descriptive Statistic

```{r, results='hide', message=FALSE}
attach(stroke_data)
```

In order to highlight and study better the data, we used some plots to study their statistics and distribution.
A relevant and important information is provided by the following barplot, in which we see an unbalance dataset issue: 209 people on a total of 4909 get a stroke, i.e. the 4.25 % of the people.
```{r, fig.height=3, fig.width=3, fig.align='center'}
table(stroke)/dim(stroke_data)[1]
barplot(table(stroke)/dim(stroke_data)[1],
        xlab='probability to have a stroke',col = c('#F8766D','#00BFC4'))
```
This value is representative of the real situation in which there are not many stroke cases compared with the whole population. The incidence of stroke in Europe at the beginning of the 21st century varies from 95 to 290 cases/100,000^[QUADERNI dell'Italian Journal of Medicine, A Journal of Hospital and Internal Medicine, Michele Meschi,volume 8, issue 2, March-April 2020]. Furthermore in many clinical diseases analysis this issue is commonly present.

A visual transformation of the values seen in the `summary` function is provided in the following boxplots:
```{r, fig.height=3.5, fig.width=6.5}
par(mfrow=c(1,3))
boxplot(avg_glucose_level, xlab= 'average glucose level' )
boxplot(bmi, xlab = 'body mass index')
boxplot(age, xlab = 'age',pch=20)
par(mfrow=c(1,1))
```

From above we can see that in the first two boxplots (starting from the left) there are lots of outliers, that can also be see from the summary looking at the difference between the third quantile and the maximum value in the `avg_glucose_level` and `bmi` variables.
Actually, they represent real-scenario (few people are affected by high glucose levels) and possible interesting cases of pathologies bounded with diabetes. Hence these data points have to be considered during modeling, they could be helpful to predict stroke cases since as mentioned in the medical literature stroke could be also due to complications of diabetes.

In order to compare entities in pairs and judge which of each entity is preferred, or has a greater amount of some quantitative property we provide a pair-wise plot. In addition, to involve also the categorical variables we wrote some useful function:
```{r}
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...){
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- abs(cor(x, y))
  txt <- format(c(r, 0.123456789), digits = digits)[1]
  txt <- paste0(prefix, txt)
  if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
  text(0.5, 0.5, txt, cex = cex.cor * r)
}

panel.hist <- function(x, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col = "green", ...)
}

box_plot_categories <- function(data, y){
  n_features = length(data)
  grid = round(sqrt(n_features))
  print(grid)
  par(mfrow=c(grid, grid))
  names = colnames(data)
  for (idx in c(1:n_features)) {
    plot(y ~ data[, idx], xlab=names[idx], main=c('Boxplot y ~ ', names[idx]))
  }
  par(mfrow=c(1, 1))
}
```

And here we show the results from the pairs plot:
```{r}
pairs(stroke_data, diag.panel=panel.hist, upper.panel=panel.cor)
```

The plot shows that the stronger relationships involve quite often the variable `age`. There are also other relevant relation between `work_type` and `ever_married` plus `bmi` with `working_status`.
In addition, we can see strong collinearity among th dummy variables `age`, `work_type`, and `ever_married`, so they do not contribute in the fitting.

We go on looking at some intuitive relation of `stroke` with `age`,`bmi` and `avg_glucose_level`:
```{r, fig.height=3.5, fig.width=6.5}
par(mfrow=c(1,3))
boxplot(avg_glucose_level~stroke, xlab= 'stroke', 
        ylab = 'average glucose level', col = c('#F8766D','#00BFC4'))
boxplot(bmi~stroke, xlab = 'stroke', ylab = 'bmi',col = c('#F8766D','#00BFC4'))
boxplot(age~stroke, xlab='stroke' ,ylab = 'age',col = c('#F8766D','#00BFC4'))
#boxplot(heart_disease~stroke, xlab='stroke' ,ylab = 'heart_disease')
par(mfrow=c(1,1))
```

Looking at these plots we can see that the incidence of the disease increases progressively with age and that if we sum also the information about `avg_glucose_level` we may wonder if diabetic people are more probable to get a stroke or not. In addition there is no apparent relation of `stroke` with `bmi`.

We now highlight other visual relationship between the variables used before, thanks to the scatter plots:
```{r,fig.height=3, fig.width=4.3, fig.align='center'}
library(ggplot2)
ggplot(stroke_data, aes(x = avg_glucose_level, y = bmi,col = as.factor(stroke))) +
  labs(x = "average glucose level", y = "bmi", color = "Stroke") + geom_point()
ggplot(stroke_data, aes(x = avg_glucose_level, y = age,col = as.factor(stroke))) + 
  labs(x = "average glucose level",y = "age", color = "Stroke") + geom_point()
ggplot(stroke_data, aes(x = bmi, y = age, col = as.factor(stroke))) +
  labs(x = "bmi", y = "age", color = "Stroke") +geom_point()
```

Difficult to read data and give a clear classification/explanation. 

Even with high `avg_glucose_level` and `bmi` it's not so straightforward to detect the stroke, since they could not be so strictly related to disease but maybe correlated to other illnesses linked (or not) to it. In the end it seem to be not so simple to identify the direct relationship with the stroke while dealing with the features that we have.

*be related to other disease (which are not correlated with stroke????) or there are still not enough complications to develop a stroke. Insomma l'associazione non e` cosi diretta e il problema sembra essere complicato perche non descrive una chiara classificazione guardando i punti.*

At this point we can ask some questions:

* Is it possible to prevent ictus?
* Which factors are the most related to it?
* How strong are the relations between the features?
* Are the given variables enough to predict a good accuracy of some possible person affected by ictus?

We will explore the data trying to answer them.

# 3 Modeling

**exp(beta) and odds for the prediction of success**

We will now present some different approaches for the classification of the data.

## 3.1 Logistic Regression

In this part of the predictive analysis we will present three different type of models, compared to discover the best one that can better interpret the data.

While going on with the classification using logistic regression we could meet the following problems: non-linearity of the data, correlation of error terms, heteroschedasticity, outliers, leverage point and collinearity.

### 3.1.1 Full and Reduced Models

We start with the full model to see if all the features of the dataset contribute positively/negatively on the prediction of a stroke.
```{r, fig.dim=c(3.5,3.5)}
mod.full <- glm(stroke~., data=stroke_data, family = binomial)
summary(mod.full)
```
Here we see that `age`, `avg_glucose_level` and `hypertension` are the variables most related to `stroke`.

Let's use the residual plots to get more information about this model (we use `type="deviance"` because we have a binary response):
```{r}
mod.full.resid <- residuals(mod.full, type="deviance") 
predicted <- predict(mod.full, type = "link")
par(mfrow=c(1,2))
plot(mod.full.resid~predicted)
abline(h=0, col='red')
qqnorm(mod.full.resid)
qqline(mod.full.resid, col='red')
par(mfrow=c(1,1))
```
The residual plots are not satisfactory because it is not easy to interpret them. From the right plot we can see that the data are not normal.

We now make some test in order to find the best reduced model: we start from the full model and then remove all the features that have collinearity between each other,i.e. `work_type`, `Residence_type` and `ever_married`.
```{r, results='hide'}
mod.red1 <- glm(stroke ~ age + bmi + avg_glucose_level + hypertension + 
                  smoking_status + gender + heart_disease, family=binomial)
```

Also in this case the variables more important are the same of the ones found in the full moel but also `heart_disease` seems to contribute to the prediction.
In this step we try to remove the `gender` variable:
```{r, results='hide'}
mod.red2 <- glm(stroke ~ age + bmi + avg_glucose_level + hypertension + 
                  smoking_status  + heart_disease, family=binomial) 
summary(mod.red2)
```

The variables left to remove are `bmi` and `smoking_status`, and we ended up with the final reduced model:
$$\verb|stroke| = \beta_0 + \beta_1\times \verb|age| + \beta_2\times \verb|heart_disease| + \beta_3 \times  \verb|avg_glucose_level| + \beta_4 \times\verb|hypertension|$$

```{r}
mod.red <- glm(stroke~age + heart_disease + avg_glucose_level+ hypertension, 
               data=stroke_data, family = binomial)
summary(mod.red)
```
For this model we also give a descriptive statistic through the residual plots:
```{r}
mod.red.resid <- residuals(mod.red, type="deviance")
predicted <- predict(mod.red, type = "link")
par(mfrow=c(1,2))
plot(mod.red.resid~predicted)
abline(h=0, col='red')
qqnorm(mod.red.resid)
qqline(mod.red.resid, col='red')
par(mfrow=c(1,1))
```

We can see from the residual vs predicted values the presence of high non-linearity in the dataset.
In the qqplot instead we see that residuals do no follow a normal distribution.

Positive coefficient estimates ---> positive association. So, the larger the value, the higher is the estimated probability of stroke.

Instead in the standard deviance vs predicted we can see that homoscedasticity does not hold since the line of the standard residual is not flat, hence even by standardizing the residual we end up having high variance among residuals.

In the end by looking at the leverage plot, we see the presence of some sample with high leverage values (bottom right), which could influence the prediction of the model. 

*Buuut I don't know in which range of levarage value is considered to change a lot the prediction of the model.*
Furthermore R does not show the index of the sample with high leverage, *I guess because a lot of values could change the prediction.*

Some outliers with high variance are: idx: 119, 183, 246.

In the end, if we compare the results obtained from the full and reduced models we can say that the reduced seems to make a more accurate prediction and fits better the data, also the AIC is lower than the one of the full model. To have a confirmation of this, we use the anova function, which compare the two models and returns the better between the two.
```{r}
anova(mod.full, mod.red, test="Chisq")
```
As expected from the anova test rejects that the complex model is more significant than the reduced one, since the p-value is not less than 5%. Hence the full model does not help with our prediction.

Outliers of the reduced model:
```{r, echo=FALSE}
kable(stroke_data[c("119","183","246"),], format = 'simple', align='cccccccc', 
      col.names = c('gender','age', 'hypert.', 'hd' ,'ev_marr',
                    'work_type','res_type','glucose', 'bmi','smoking','stroke'))
```


### 3.1.2 Interaction

In order to see which variables were relevant on our research we tested various models using the mixed approach: we started by the reduced model `mod.red` and then tested some interaction between the explanatory variables for improving the performance of the model.

We started with `mod.red`, which had `stroke` as response and `age`, `avg_glucose_level`,`hypertension` and `heart_disease` as predictors.
We recall that its AIC was 1384.6.
**commento sul threshold --> teniamo il 90%**
We then consider the interaction of `age` with the other numerical features, i.e. `avg_glucose_level`,`hypertension` and `heart_disease`: we find out that only `age*heart_disease` was relevant between the ones tested, with an AIC = 1384, which was lower than the one of the reduced model `mod.red`. 
```{r}
mod1 <- glm(stroke~age + avg_glucose_level+ heart_disease+ hypertension +
               age*heart_disease, family=binomial)
summary(mod1)
```
We went on considering all the interaction of `avg_glucose_level` with the remaining predictors and find out that `avg_glucose_level*hypertension` was the best of the possible interaction but cannot improve the previous model, it had an AIC of 1385.9.
```{r, results='hide'}
mod2 <- glm(stroke~age + avg_glucose_level+ heart_disease+hypertension +
              avg_glucose_level*hypertension, family=binomial)
summary(mod2)
```
In the end it was left the interaction `heart_disease*hypertension` which return an AIC 1384.5 for the model.
```{r, echo = FALSE}
mod3 <- glm(stroke~age + avg_glucose_level+ heart_disease+hypertension + 
            heart_disease*hypertension, family=binomial)
summary(mod3)
```

We then tried to see the results of the `mod.red` without `heart_disease` which was the explanatory variables with higher p-value, and then we added some interaction terms, starting with the one with `age`:
```{r, results='hide'}
mod4 <- glm(stroke~age + avg_glucose_level + hypertension + 
              age*hypertension, family=binomial)
summary(mod4)
```
The AIC of this model was 1386.2, higher that the ones seen on the previous tested models.
We go further testing also the interaction add `avg_glucose_level*hypertension`
```{r, results='hide'}
mod5 <- glm(stroke~age + avg_glucose_level + hypertension + 
              avg_glucose_level*hypertension, family=binomial)
summary(mod5)
```
It had an AIC = 1387.6.
In the end we return on our base model, `mod.red` and add the teo best iteractions, i.e. the two terms that reduced the AIC term:
```{r, results='hide'}
mod6 <- glm(stroke~age + avg_glucose_level+ heart_disease+ hypertension +
              age*heart_disease + heart_disease*hypertension, family=binomial)
summary(mod6)
```
It has AIC = 1384.2 but the interactions had a p-value greater than 0.1 and it didn't seem to represent our data properly.
At the end of all we promote `mod1` as the model which better fits our data.

Let's now see some relevant information, such as outliers on the `mod1`:
```{r, echo=FALSE}
kable(stroke_data[c("207","150","100"), ], format = 'simple', align='cccccccc', 
      col.names = c('gender','age', 'hypert.', 'hd' ,'ev_marr',
                    'work_type','res_type','glucose', 'bmi','smoking','stroke'))
```

but also leverage point and collinearity:
```{r,fig.dim=c(3.5,3.5)}
plot(mod1)
```

### 3.1.3 Polynomial models

We tried to use a polynomial model starting from the reduced model `mod.red` with also `bmi` as predictors. Then we contribute with the square of `bmi`, `avg_glucose_level` and then both for the `mod.poly1`,`mod.poly2` and `mod.poly3` respectively.
```{r, results='hide'}
mod.poly1 <- glm(stroke~age + heart_disease + avg_glucose_level+ hypertension+bmi+
                       I(bmi^2), family = binomial)
summary(mod.poly1)
mod.poly2 <- glm(stroke~age + heart_disease + avg_glucose_level+ hypertension+bmi+
                     + I(avg_glucose_level^2), family = binomial)
summary(mod.poly2)
mod.poly3 <- glm(stroke~age + heart_disease + avg_glucose_level+ hypertension+bmi+
                      I(bmi^2) + I(avg_glucose_level^2), family = binomial)
summary(mod.poly3)
```
At the end of the tests nothing interesting appeared with polynomial models. There were no improvement in the results.

## 3.2 LDA

Assumption: samples are normally distributed and have same variance in every class => strong assumption.
```{r}
library(MASS)
lda.fit <- lda(stroke~age+bmi+avg_glucose_level+hypertension+work_type+gender
               +smoking_status+ever_married+Residence_type + heart_disease)
lda.pred <- predict(lda.fit)
table(lda.pred$class, stroke)
lda.pred.stroke <- lda.pred$posterior[, 2]
```

## 3.3 QDA

Assumption: sample are normally distributed BUT NOT SAME variance among classes.
```{r}
qda.fit <- qda(stroke~age+bmi+avg_glucose_level+hypertension+heart_disease+smoking_status, data = stroke_data)
# ERROR rank deficiency, i.e. some variables 
# are collinear and one or more covariance matrices cannot be inverted to obtain the estimates in group 1 (Controls)!
qda.pred <- predict(qda.fit, stroke_data)
qda.pred.stroke <- qda.pred$posterior[, 2]
table(qda.pred$class, stroke)
```

# 4 Predictions

In order to split the dataset into validation, training and test sets we recall that the amount of data that we have is of 4909, so we decided to keep approximately the 75% of the data for the training phase: 3682 people of which 3562 are the ones which hadn't the stroke, while 120 had it.
We didn't use the cross-validation because it was difficult to split tha data and bla bla.

**CODICE CORRETTO**

## 4.1 ROC and PRECISION-RECALL Curves

We introduced an hand-written function to make usefull plot because 
Allora ieri guardando i plot della ROC curve io e francesca c'eravamo posti due domande sui risultati.
Siccome siamo in un problema medico di predizione di ictus di un paziente oppure no, alla fine la ROC curve non e` molto d'aiuto perche massimizza i true positive con i true predicted. 
Questo significa che possibilmenete gli errori di false negativo possono incrementare.
In ambito del nostro problema e in generale in ambito medico se il nostro modello predice una persona senza ictus quando invece lo presenta, eh  questa e` un errore piu grave rispetto a un false positive.
Noi vorremmo invece che il false negative sia basso e quindi considerato. 
Insomma significa che dobbiamo usare la Precision Recall curve.
```{r}
library(pROC)
library(ROCR)
get.roc.recall.values <- function(pred_models, true_value) {
  result <- data.frame(Threshold1=double(), Specificity=double(), Sensitivity=double(),
                       Threshold2=double(), Recall=double(), Precision=double())
  n_models = length(list(mod.red.probs,lda.pred.stroke, qda.pred.stroke))
  par(mfrow=c(n_models, 2))
  for (pred in pred_models) {
    roc.res <- roc(true_value, pred, levels=c("0", "1"))
    plot(roc.res, print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", 
         ylab="True positive rate")
    
    tmp.res  <- coords(roc.res, "best")
    pred.rec = prediction(mod.red.probs, true_value)
    perf = performance(pred.rec, "prec", "rec")
    plot(perf)
    pr_cutoffs <- data.frame(cutrecall=perf@alpha.values[[1]], recall=perf@x.values[[1]], 
                             precision=perf@y.values[[1]])
    best_recall <- pr_cutoffs[which.min(pr_cutoffs$recall + pr_cutoffs$precision), ]
    
    result[nrow(result) + 1,] = c(tmp.res[1, 1], tmp.res[1, 2], tmp.res[1, 3], 
                                  best_recall[1, 1], best_recall[1, 2], best_recall[1, 3])
  }
  par(mfrow=c(1, 1))
  return(result)
}
mod.red <- glm(stroke~age + avg_glucose_level + hypertension + bmi, data=stroke_data, 
               family = binomial)
summary(mod.red)
mod.red.probs <- predict(mod.red,type="response")
```

## 4.2 Training Set

Insertion of the no-stroke people into the training set:
```{r}
no.strokes.data <- stroke_data[stroke == 0, ]
rnd.idx.no.strokes <- sample(c(1:dim(no.strokes.data)[1]))
```

Insertion of the stroke people into the training set:
```{r}
yes.strokes.data <- stroke_data[stroke == 1, ]
rnd.idx.yes.strokes <- sample(c(1:dim(yes.strokes.data)[1]))
```

We mix together the two parts and we use the `shuffle` function because the strokes are added in the last positions
```{r}
training.set <- no.strokes.data[rnd.idx.no.strokes[1:3562], ]
training.set <- rbind(training.set, yes.strokes.data[rnd.idx.yes.strokes[1:120], ])
shuffle <- sample(nrow(training.set))
training.set <- training.set[shuffle, ]
```


## 4.3 Validation Set

We mix shuffle together the remaining samples into forming the validation set.
```{r}
val.set <- no.strokes.data[rnd.idx.no.strokes[3563:4700], ]
val.set <- rbind(val.set, yes.strokes.data[rnd.idx.yes.strokes[121:209], ])
shuffle <- sample(nrow(val.set)) 
val.set <- val.set[shuffle, ]
```


# 5 Conclusions

Si stima che la percentuale di persone che possono avere un ictus andrà via via crescendo dal momento che l'età media della popolazione è in costante crescita.