---
title: "Statistical Learning Project"
author: " Filippo Santin, Gurjeet Singh, Francesca Zen"
date: "18/5/2021"
output:
  pdf_document: default
  html_document: default
---
# 1. Introduction

In the following report we present the analysis computed on a particular disease, stroke, and its correlation to other factors such as smoking, glucose level, bmi and so on. 

"Stroke" is the medical term for damage to brain tissue or the death of a portion of it, due to insufficient blood supply to an area of the brain.

Our aim is to see if and how the variables we are dealing with are related to each other, in order to predict which individual is more probable to get a stroke.

The symptoms of stroke vary from patient to patient, depending on the severity of the condition, the affected brain area, causes, type of stroke, etc.

Stroke is characterized by sudden onset and for this reason it involves the need for immediate therapeutic intervention and adapted to the needs of the patient.
In this sense, looking for relation between features may help to prevent it.

In order to have a guide for the interpretation of the data we underline the following information:

* The normal values of glucose level are between 60 and 110 mg/dl and with a value greater than 126 mg/dl a person is considered diabetic;
* a body mass index (BMI) between 18.5-24.9 indicates a normal/healthy weight, below 18.5 indicates underweight, 25.0-29.9 indicates overweight and above 30.0 indicates obese person 

# 2. Exploiting the Dataset

 The dataset we used was provided by kaggle ^[https://www.kaggle.com/fedesoriano/stroke-prediction-dataset] and it is composed of 5,110 entries with a total of 12 columns: `id`, `gender`, `age`, `hypertension`, `heart_disease`, `ever_married`, `work_type`, `Residence_type`, `avg_glucose_level`, `bmi`, `smoking_status`, `stroke`.
```{r}
stroke_data <- read.csv('healthcare-dataset-stroke-data.csv')
attach(stroke_data)
```

The preliminary part of the analysis focuses on the study of the dataset: we looked at the `id` column and verified that all the data collected was referring to different people, so no recidivist status were involved.
```{r}
stroke_data<-stroke_data[,-1]
```

In order to use the variables through the analysis we then transformed the categorical variables into factors:

```{r}
stroke_data$gender<- as.factor(gender)
stroke_data$ever_married<-as.factor(ever_married)
stroke_data$work_type<-as.factor(work_type)
stroke_data$Residence_type<-as.factor(Residence_type)
stroke_data$smoking_status<-as.factor(smoking_status)
```

What's more, the variable `bmi` was not numeric because of the presence of a string "N/A" which identifies the lack of the value, and so we transformed it using the command `stroke_data$bmi <- as.numeric(bmi)`; and then removed those NA values:`stroke_data<- na.omit(stroke_data)`.

```{r,echo=FALSE, results='hide', message=FALSE}
stroke_data$bmi <- as.numeric(bmi)
stroke_data<- na.omit(stroke_data)
```

We ended up having  4,909 entries and 11 total columns.
Here we give a quick overview of the main information about the dataset:
```{r}
summary(stroke_data)
```

```{r, results='hide', message=FALSE}
attach(stroke_data)
```

A visual transformation of these values is provided in the following boxplots:
```{r, fig.height=3, fig.width=6}
par(mfrow=c(1,3))
boxplot(avg_glucose_level, xlab= 'average glucose level' )
boxplot(bmi, xlab = 'body mass index')
boxplot(age, xlab = 'age',pch=20)
```
```{r, echo=FALSE}
par(mfrow=c(1,1))
```

Through the analysis on the Stroke dataset, we discovered that it was strongly bias, in the sense that 209 people on a total of 4909 get a stroke:
```{r, fig.height=3, fig.width=3}
barplot(table(stroke)/dim(stroke_data)[1],
        xlab='probability to have a stroke')
```

This value is representative of the real situation in which there are not many stroke cases compared with the whole population. In Italy, for example, we have 200,000 cases over 59.226.539 people, i.e. 0.33%.

# 3. Searching for Relationships
At this point we can ask some questions:

* Is it possible to prevent ictus?
* Which factors are the most related to it?
* How strong are the relations between the features?
* Are the given variables enough to predict a good accuracy of some possible person affected by ictus?

We will explore the data trying to answer them.

First of all we look at some intuitive relation between `stroke` and `age`,`bmi` and `avg_glucose_level`:
```{r, echo=FALSE}
par(mfrow=c(3,1))
```

```{r,fig.height=3, fig.width=4.5}
plot(stroke~age)
plot(stroke~bmi)
plot(stroke~avg_glucose_level)
```
```{r, echo=FALSE}
par(mfrow=c(1,1))
```
Looking to these plots we can see that the incidence of the virus increases progressively with age and that if we sum also the information about `avg_glucose_level` we may wonder if diabetic people are more probable to get a stroke or not. In addition there is no apparent relation of `stroke` with `bmi`.

We now highlight other visual relationship thanks to the scatter plots:
```{r,fig.height=3, fig.width=4}
library(ggplot2)
par(mfrow=c(1,3))
ggplot(stroke_data, aes(x = avg_glucose_level, y = bmi,
                        col = as.factor(stroke))) + geom_point()
ggplot(stroke_data, aes(x = avg_glucose_level, y = age,
                        col = as.factor(stroke))) + geom_point()
ggplot(stroke_data, aes(x = bmi, y = age,
                        col = as.factor(stroke))) + geom_point()
par(mfrow=c(1,1))
```
**discussione**

# 4. Analysis on models

While going on with the classification using logistic regression we could meet the following problems: non-linearity of the data, correlation of errorterms, heteroschedasticity, outliers, leverage point and collinearity.

## 4.1 Full and Reduced Models

```{r}
mod.full <- glm(stroke~., data=stroke_data, family = binomial)
summary(mod.full)
```
Here we see that `age` and `hypertension` are the variables most related to `stroke`.

Let's use the residual plots to get more information about this model:
```{r}
mod.full.resid <- residuals(mod.full, type="deviance") # because we have a binary response
predicted <- predict(mod.full, type = "link")
par(mfrow=c(1,2))
plot(mod.full.resid~predicted)
abline(h=0, col='red')
qqnorm(mod.full.resid)
qqline(mod.full.resid, col='red')
```
The residual plots aare not satisfactory. From the right plot we can see that the data are not normal.

## 4.2 Comparison between Reduced Models

We remove at least all the features that have collinearity between each other (`work_type`, `ever_married`) and the `Residence_type`
```{r}
mod.red1 <- glm(stroke ~ age + bmi + avg_glucose_level + hypertension + smoking_status + gender + heart_disease, family=binomial) #
summary(mod.red1)

# Reduced model 2, Remove gender
mod.red2 <- glm(stroke ~ age + bmi + avg_glucose_level + hypertension + smoking_status  + heart_disease, family=binomial) #
summary(mod.red2)

#####
mod.red <- glm(stroke~age + heart_disease + avg_glucose_level+ hypertension, data=stroke_data, family = binomial)
summary(mod.red)
summary(mod.red)
```
The `mod.red` has `age`, `hypertension`, `avg_glucose_level` as the variables with the highest level of significance.

```{r}
mod.red.resid <- residuals(mod.red, type="deviance")
predicted <- predict(mod.red, type = "link")

par(mfrow=c(1,2))
plot(mod.red.resid~predicted)
abline(h=0, col='red')
qqnorm(mod.red.resid)
qqline(mod.red.resid, col='red')
```

Residual plot:
```{r,fig.dim=c(1,1)}
#par(mfrow=c(2,2))
#plot(mod.red)
#par(mfrow=c(1,1))
```
We can see from the residual vs predicted values the presence of high non-linearity in the dataset.
In the qqplot instead we see that residuals do no follow a normal distribution.

Instead in the standard deviance vs predicted we can see that homoscedasticity does not hold since the line of the standard residual is not flat, hence even by standardizing the residual we end up having high variance among residuals.

In the end by looking at the leverage plot, we see the presence of some sample with high leverage values (bottom right), which could influence the prediction of the model. 

*Buuut I don't know in which range of levarage value is considered to change a lot the prediction of the model.*
Furthermore R does not show the index of the sample with high leverage, *I guess because a lot of values could change the prediction.*
Some outliers with high variance are: idx: 119, 183, 246.

Anova computation:
```{r}
anova(mod.full, mod.red, test="Chisq")
```
As expected from the anova test rejects that the complex model is more significant than the reduced one, since the p-value is not less than 5%. Hence the full model does not help with our prediction.

Outliers:
```{r}
View(stroke_data[c("119","183","246"), ])
```


## 4.3 Mixed Approach for Models

In order to see which variables were relevant on our research we tested various models usign the mixed approach: we started by the null model and then added variables and removed those with p-value over a threshold of 0.1 (which indicates low relationship with the `stroke` variable). In addition, we used the F-statistic to determine those variables that were in good relation, so they may contribute positively to the models:
```{r, echo=FALSE}
var.test(age,avg_glucose_level) 
var.test(age, hypertension) 
var.test(hypertension,avg_glucose_level)
var.test(age, heart_disease)
var.test(avg_glucose_level,heart_disease)
var.test(age,bmi) 
```
All the p-values of the tests were above 2.2e-16 and so each of them were good relationships. Only the value of the F-statitic varied.

We start with a simple model with `stroke` as response and `age`, `avg_glucose_level`,`hypertension` and `heart_disease` as predictors.
```{r}
mod1 <- glm(stroke~age+avg_glucose_level+hypertension+heart_disease, family=binomial)
summary(mod1)
```
We then tested some iterations between the explanatory variables is order to see if we can improve our model:
```{r, results='hide'}
mod2 <- glm(stroke~age+avg_glucose_level+hypertension+heart_disease + age*avg_glucose_level, family = binomial)
summary(mod2)
```
`avg_glucose_level*age` and `avg_glucose_level` have p-value over the threshold, so we remove them, then add `hypertension`:
```{r, results='hide'}
mod3 <-glm(stroke ~ age + hypertension, family = binomial)
summary(mod3) 
```
All the p-values makes the variables relevant so we keep all of them and we add the interaction between `hypertension` and `glucose_level` and `age`:
```{r, echo=FALSE}
mod4 <- glm(stroke~age+hypertension+ hypertension*avg_glucose_level+hypertension*age,
             family = binomial)
summary(mod4)
```
We remove `hypertension*avg_glucose_level` and add `heart_disease`:

```{r, results='hide'}
mod5 <- glm(stroke~age + hypertension +heart_disease, family = binomial)
summary(mod5)
```
We add `heart_disease *avg_glucose_level + heart_disease*hypertension + heart_disease*age` 
```{r, echo=FALSE}
mod6 <- glm(stroke~age + hypertension + heart_disease + heart_disease*avg_glucose_level+
              heart_disease*hypertension+ heart_disease*age, family = binomial)
summary(mod6)
```
We can remove `heart_disease` because it has a p-value of 0.1925 but also `heart_disease*hypertension` and `heart_disease*age`. We go on adding `bmi`:
```{r, echo=FALSE}
mod7 <- glm(stroke~age + hypertension + heart_disease*avg_glucose_level+ bmi, family = binomial)
summary(mod7)
```
We remove `bmi` because it has a p-value of 0.68868 and  also `heart_disease*avg_glucose_level` and add its relation with `age` and `hypertension` because they are the most relavant variables:
```{r, echo=FALSE}
mod8 <- glm(stroke~ age + hypertension + heart_disease*hypertension +
               heart_disease*age + bmi*age + bmi*hypertension, family = binomial)
summary(mod8)
```
 We remove them plus and left the model with:
```{r}
mod9 <- glm(stroke~ age + avg_glucose_level + hypertension+ heart_disease*age, 
            family = binomial)
summary(mod9)
```

We can also try with the categorical variables but actually the results do not change, so we keep this `mod9`.

Let's now see some relevant information, such as outliers, leverage point and collinearity through some plots:
```{r,fig.height=3, fig.width=4}
par(mfrow=c(2,2))
#plot(mod9)
par(mfrow=c(1,1))
```

## 4.4 Polynomial models

We try to use a polynomial model using the numerical variables with degree 2.
```{r}
mod.red.poly1 <- glm(stroke~age + bmi + avg_glucose_level+ hypertension+
                       I(bmi^2), family = binomial)
summary(mod.red.poly1)

mod.red.poly2 <- glm(stroke~age + bmi + avg_glucose_level+ hypertension
                     + I(avg_glucose_level^2), family = binomial)
summary(mod.red.poly2)

mod.red.poly <- glm(stroke~age + bmi + avg_glucose_level+ hypertension+
                      I(bmi^2) + I(avg_glucose_level^2), family = binomial)
summary(mod.red.poly)
```
Nothing interesting with polynomial models. No improvement in the results.

# 5. LDA

Assumption: samples are normally distributed and have same variance in every class => strong assumption.
```{r}
library(MASS)
lda.fit <- lda(stroke~age+bmi+avg_glucose_level+hypertension+work_type+gender
               +smoking_status+ever_married+Residence_type + heart_disease)
lda.pred <- predict(lda.fit)
table(lda.pred$class, stroke)
lda.pred.stroke <- lda.pred$posterior[, 2]
```

# 6. QDA

Assumption: sample are normally distributed BUT NOT SAME variance among classes.
```{r}
qda.fit <- qda(stroke~age+bmi+avg_glucose_level+hypertension+heart_disease+smoking_status, data = stroke_data)
# ERROR rank deficiency, i.e. some variables 
# are collinear and one or more covariance matrices cannot be inverted to obtain the estimates in group 1 (Controls)!
qda.pred <- predict(qda.fit, stroke_data)
qda.pred.stroke <- qda.pred$posterior[, 2]

table(qda.pred$class, stroke)
```

# 7. ROC and RECALL-PRECISION CURVES

```{r}
library(pROC)
library(ROCR)
get.roc.recall.values <- function(pred_models, true_value) {
  result <- data.frame(Threshold1=double(), Specificity=double(), Sensitivity=double(),
                       Threshold2=double(), Recall=double(), Precision=double())
  n_models = length(list(mod.red.probs,lda.pred.stroke, qda.pred.stroke))
  par(mfrow=c(n_models, 2))
  for (pred in pred_models) {
    roc.res <- roc(true_value, pred, levels=c("0", "1"))
    plot(roc.res, print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")
    
    tmp.res  <- coords(roc.res, "best")
    pred.rec = prediction(mod.red.probs, true_value)
    perf = performance(pred.rec, "prec", "rec")
    plot(perf)
    pr_cutoffs <- data.frame(cutrecall=perf@alpha.values[[1]], recall=perf@x.values[[1]], 
                             precision=perf@y.values[[1]])
    best_recall <- pr_cutoffs[which.min(pr_cutoffs$recall + pr_cutoffs$precision), ]
    
    result[nrow(result) + 1,] = c(tmp.res[1, 1], tmp.res[1, 2], tmp.res[1, 3], 
                                  best_recall[1, 1], best_recall[1, 2], best_recall[1, 3])
  }
  par(mfrow=c(1, 1))
  return(result)
}


mod.red <- glm(stroke~age + avg_glucose_level + hypertension + bmi, data=stroke_data, family = binomial)
summary(mod.red)
mod.red.probs <- predict(mod.red,type="response")
```


Si stima che la percentuale di persone che possono avere un ictus andrà via via crescendo dal momento che l'età media della popolazione è in costante crescita.
